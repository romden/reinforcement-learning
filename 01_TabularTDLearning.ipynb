{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular methods for temporal-difference learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Richard S. Sutton and Andrew G. Barto (2018). *Reinforcement Learning: An Introduction*. A Bradford Book, Cambridge, MA, USA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The performance of the algorithms can be improved by adjusting parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 On policy learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tabular-sarsa.png\" width=\"800\" height=\"800\" style=\"float: left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "\n",
    "    def __init__(self, env, num_grids):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        # discretize state space putting a grid\n",
    "        self.grid_size = (env.observation_space.high - env.observation_space.low) / num_grids        \n",
    "\n",
    "        # numpy array of size (d1,d2,...,dn)\n",
    "        table_size = tuple(num_grids) + (env.action_space.n,)\n",
    "        self.q_table = np.zeros(table_size)\n",
    "        \n",
    "        # learning parameters\n",
    "        self.num_episodes = 5000\n",
    "        self.epsilon = 0.3\n",
    "        self.alpha = 0.1 # learning rate\n",
    "        self.gamma = 0.99 # discount rate\n",
    "        \n",
    "        self.verbose = 200\n",
    "        self.to_show = 500\n",
    "        self.stats = {'rewards': []}        \n",
    "\n",
    "    def get_discrete_state(self, state):\n",
    "        discrete_state = (state - self.env.observation_space.low) / self.grid_size\n",
    "        return tuple(discrete_state.astype(np.int))  # returns tuple of integers\n",
    "\n",
    "    def act(self):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            discrete_state = self.get_discrete_state(state)            \n",
    "            q_values = self.q_table[discrete_state]\n",
    "            action = self.epsilon_greedy_selection(q_values, epsilon=0)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            self.env.render()\n",
    "        self.env.close()\n",
    "            \n",
    "    def epsilon_greedy_selection(self, q_values, epsilon):\n",
    "        # select action\n",
    "        if np.random.random() < epsilon:  # Exploration: randomly selected action\n",
    "            action = np.random.randint(0, len(q_values))\n",
    "\n",
    "        else:  # Greedy selection: action with max Q(s,a)\n",
    "            q_max = np.max(q_values)\n",
    "            q_max_actions = [i for i, q in enumerate(q_values) if q == q_max]\n",
    "            action = q_max_actions[np.random.randint(0, len(q_max_actions))]\n",
    "        return action\n",
    "\n",
    "    def execute(self):\n",
    "\n",
    "        # for each episode\n",
    "        for episode in range(self.num_episodes):\n",
    "            \n",
    "            # init episode stats\n",
    "            self.stats['rewards'].append(0)          \n",
    "            \n",
    "            # get initial state\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # discretize current state\n",
    "            state = self.get_discrete_state(state)\n",
    "\n",
    "            # select action based on Q(s,a) for the current state\n",
    "            action = self.epsilon_greedy_selection(self.q_table[state], self.epsilon)\n",
    "\n",
    "            # for each time step in the episode (or until terminal state)\n",
    "            while True:\n",
    "\n",
    "                # carry out action, observe new state and reward\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # discretize new state\n",
    "                new_state = self.get_discrete_state(new_state)\n",
    "\n",
    "                # select action for new state\n",
    "                new_action = self.epsilon_greedy_selection(self.q_table[new_state], self.epsilon)\n",
    "\n",
    "                # compute Q(s_new,a) in the new state\n",
    "                if done:\n",
    "                    q_new = reward\n",
    "                else:\n",
    "                    q_new = reward + self.gamma * self.q_table[new_state][new_action]\n",
    "                \n",
    "                # index to look up q_table\n",
    "                idx = state + (action,)\n",
    "                \n",
    "                # get current Q(s,a)\n",
    "                q_old = self.q_table[idx]\n",
    "\n",
    "                # update\n",
    "                self.q_table[idx] = q_old + self.alpha * (q_new - q_old)\n",
    "\n",
    "                # update episode stats\n",
    "                self.stats['rewards'][-1] += reward\n",
    "\n",
    "                # decide whether to continue episode\n",
    "                if done:\n",
    "                    break\n",
    "                else:  # update\n",
    "                    state = new_state\n",
    "                    action = new_action\n",
    "\n",
    "            # print stats on screen\n",
    "            if self.verbose > 0 and episode % self.verbose == 0:\n",
    "                print(\"episode: {}\\t reward: {:.2f}\".format(episode, self.stats['rewards'][-1]))\n",
    "                \n",
    "            # show simulated environment with learned parameters\n",
    "            if self.to_show > 0 and episode % self.to_show == 0:\n",
    "                self.act()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\t reward: -200.00\n",
      "episode: 200\t reward: -200.00\n",
      "episode: 400\t reward: -200.00\n",
      "episode: 600\t reward: -200.00\n",
      "episode: 800\t reward: -200.00\n",
      "episode: 1000\t reward: -200.00\n",
      "episode: 1200\t reward: -200.00\n",
      "episode: 1400\t reward: -200.00\n",
      "episode: 1600\t reward: -200.00\n",
      "episode: 1800\t reward: -200.00\n",
      "episode: 2000\t reward: -200.00\n",
      "episode: 2200\t reward: -200.00\n",
      "episode: 2400\t reward: -200.00\n",
      "episode: 2600\t reward: -200.00\n",
      "episode: 2800\t reward: -200.00\n",
      "episode: 3000\t reward: -200.00\n",
      "episode: 3200\t reward: -161.00\n",
      "episode: 3400\t reward: -200.00\n",
      "episode: 3600\t reward: -200.00\n",
      "episode: 3800\t reward: -199.00\n",
      "episode: 4000\t reward: -200.00\n",
      "episode: 4200\t reward: -171.00\n",
      "episode: 4400\t reward: -200.00\n",
      "episode: 4600\t reward: -200.00\n",
      "episode: 4800\t reward: -200.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "agent = SarsaAgent(env=env, num_grids=[20, 20])\n",
    "agent.execute()\n",
    "agent.act()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 Off policy learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tabular-q-learning.png\" width=\"800\" height=\"800\" style=\"float: left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "\n",
    "    def __init__(self, env, num_grids):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        # discretize state space putting a grid\n",
    "        self.grid_size = (env.observation_space.high - env.observation_space.low) / num_grids        \n",
    "\n",
    "        # numpy array of size (d1,d2,...,dn)\n",
    "        table_size = tuple(num_grids) + (env.action_space.n,)\n",
    "        self.q_table = np.zeros(table_size)\n",
    "        \n",
    "        # learning parameters\n",
    "        self.num_episodes = 5000\n",
    "        self.epsilon = 0.3\n",
    "        self.alpha = 0.1 # learning rate\n",
    "        self.gamma = 0.99 # discount rate\n",
    "        \n",
    "        self.verbose = 200\n",
    "        self.to_show = 500\n",
    "        self.stats = {'rewards': []}    \n",
    "\n",
    "    def get_discrete_state(self, state):\n",
    "        discrete_state = (state - self.env.observation_space.low) / self.grid_size\n",
    "        return tuple(discrete_state.astype(np.int))  # returns tuple of integers\n",
    "\n",
    "    def act(self):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            discrete_state = self.get_discrete_state(state)            \n",
    "            q_values = self.q_table[discrete_state]\n",
    "            action = self.epsilon_greedy_selection(q_values, epsilon=0)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            self.env.render()\n",
    "        self.env.close()\n",
    "            \n",
    "    def epsilon_greedy_selection(self, q_values, epsilon):\n",
    "        # select action\n",
    "        if np.random.random() < epsilon:  # Exploration: randomly selected action\n",
    "            action = np.random.randint(0, len(q_values))\n",
    "\n",
    "        else:  # Greedy selection: action with max Q(s,a)\n",
    "            q_max = np.max(q_values)\n",
    "            q_max_actions = [i for i, q in enumerate(q_values) if q == q_max]\n",
    "            action = q_max_actions[np.random.randint(0, len(q_max_actions))]\n",
    "        return action\n",
    "\n",
    "    def execute(self):\n",
    "\n",
    "        # for each episode\n",
    "        for episode in range(self.num_episodes):\n",
    "            \n",
    "            # init episode stats\n",
    "            self.stats['rewards'].append(0)\n",
    "            \n",
    "            # get initial state\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # discretize current state\n",
    "            state = self.get_discrete_state(state)\n",
    "\n",
    "            # for each time step in the episode (or until terminal state)\n",
    "            while True:\n",
    "\n",
    "                # select action based on Q(s,a) for the current state\n",
    "                action = self.epsilon_greedy_selection(self.q_table[state], self.epsilon)\n",
    "\n",
    "                # carry out action, observe new state and reward\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # discretize new state\n",
    "                new_state = self.get_discrete_state(new_state)\n",
    "\n",
    "                # compute Q(s_new,a) in the new state\n",
    "                if done:\n",
    "                    q_new = reward\n",
    "                else:\n",
    "                    q_new = reward + self.gamma * np.max(self.q_table[new_state])\n",
    "                \n",
    "                # index to look up q_table\n",
    "                idx = state + (action,)\n",
    "                \n",
    "                # get current Q(s,a)\n",
    "                q_old = self.q_table[idx]\n",
    "\n",
    "                # update\n",
    "                self.q_table[idx] = q_old + self.alpha * (q_new - q_old)\n",
    "\n",
    "                # update episode stats\n",
    "                self.stats['rewards'][-1] += reward\n",
    "\n",
    "                # decide whether to continue episode\n",
    "                if done:\n",
    "                    break\n",
    "                else:  # update state\n",
    "                    state = new_state\n",
    "\n",
    "            if self.verbose > 0 and episode % self.verbose == 0:\n",
    "                print(\"episode: {}\\t reward: {:.2f}\".format(episode, self.stats['rewards'][-1]))\n",
    "                \n",
    "            # show simulated environment with learned parameters\n",
    "            if self.to_show > 0 and episode % self.to_show == 0:\n",
    "                self.act()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\t reward: -200.00\n",
      "episode: 200\t reward: -200.00\n",
      "episode: 400\t reward: -200.00\n",
      "episode: 600\t reward: -200.00\n",
      "episode: 800\t reward: -200.00\n",
      "episode: 1000\t reward: -200.00\n",
      "episode: 1200\t reward: -200.00\n",
      "episode: 1400\t reward: -200.00\n",
      "episode: 1600\t reward: -200.00\n",
      "episode: 1800\t reward: -200.00\n",
      "episode: 2000\t reward: -158.00\n",
      "episode: 2200\t reward: -200.00\n",
      "episode: 2400\t reward: -200.00\n",
      "episode: 2600\t reward: -200.00\n",
      "episode: 2800\t reward: -200.00\n",
      "episode: 3000\t reward: -174.00\n",
      "episode: 3200\t reward: -200.00\n",
      "episode: 3400\t reward: -200.00\n",
      "episode: 3600\t reward: -165.00\n",
      "episode: 3800\t reward: -200.00\n",
      "episode: 4000\t reward: -200.00\n",
      "episode: 4200\t reward: -200.00\n",
      "episode: 4400\t reward: -200.00\n",
      "episode: 4600\t reward: -200.00\n",
      "episode: 4800\t reward: -200.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "agent = QLearningAgent(env=env, num_grids=[20, 20])\n",
    "agent.execute()\n",
    "agent.act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
